---
title: "Lab3/Assignment 2"
author: "Jia Lin Gao"
date: "25/08/2020"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    number_sections: yes
editor_options:
  chunk_output_type: inline
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      cache = FALSE
                      )
```

```{r}
#imports
library(tidyverse)
library(plotrix)
library(rlist)
library(gganimate)
theme_set(theme_bw())
library(av)
library(gifski)
```
# Objective and gradient functions

## Linear regression objective function

```{r echo=T}
Ein_linreg <- function(X, y, w){
  mse <- (1/nrow(X) )*t(y-X%*%w)%*%(y-X%*%w) 
  return(mse)
  }
```

## Logistic regression objective function

Equation derived from [1]

```{r echo=T}
Ein_logreg <- function(X, y, w){
  e_in <- (1/nrow(X))*sum(log(1+exp(-y*X%*%w)))
  return(e_in)
  }
```

## Linear regression gradient function

```{r echo=T}
gEin_linreg <- function(X, y, w){
  delta_Ein <- (2/nrow(X))*(t(X)%*%X%*%w-t(X)%*%y)
  return(delta_Ein)
  }
```

## Logistic regression gradient function

Equation derived from [1]

```{r echo=T}
gEin_logreg <- function(X, y, w){
    delta_Ein <- -(1/nrow(X)) * colSums(sweep(sweep(X, 1, y, "*"), 1, 1+exp(y*X%*%w), "/"))
    return(delta_Ein)
  }
```

# Gradient descent function

```{r echo=T}
GD <-function(X, y, Ein, gEin, w0, eta, precision, nb_iters){
  allw <-vector("list", nb_iters) #declare vector of weights
  cost <-numeric(nb_iters) #declare list of costs
  allw[[1]] <- w0 #initial weight
  cost[1] <-Ein(X, y, allw[[1]]) #initial cost
  flag <- F #flag for early stopping
  for (idx in seq(2, nb_iters)) 
    {
     #update costs
     allw[[idx]] <- allw[[idx-1]] - eta*gEin(X, y, allw[[idx-1]]) 
     #update cost
     cost[idx] <-Ein(X, y, allw[[idx]])
     #calculate stopping criteria
     delta <- abs(cost[idx]-cost[idx-1])
     if (precision > delta) #early stopping
     {
       #extract head of list
       res <- list(allw = allw[1:idx], cost = cost[1:idx])
       flag <- T #break loop
       break
     }
  }
  if(flag == F)
  {
    #return full list
    res <- list(allw = allw, cost = cost)
  }
  return(res)
  }

```

# Function to generate X data for linear regression

```{r echo=T}
set.seed(1900)# Function taken from Friedman et al.
genx <-function(n,p,rho){
  #    generate x's multivariate normal with equal corr rho
  # Xi = b Z + Wi, and Z, Wi are independent normal.
  # Then Var(Xi) = b^2 + 1
  #  Cov(Xi, Xj) = b^2  and so cor(Xi, Xj) = b^2 / (1+b^2) = rho
  z <-rnorm(n)
  if(abs(rho)<1){
    beta <-sqrt(rho/(1-rho))
    x <-matrix(rnorm(n*p), ncol=p)
    A <-matrix(rnorm(n), nrow=n, ncol=p, byrow=F)
    x <- beta*A+x}
  if(abs(rho)==1){ 
    x=matrix(rnorm(n),nrow=n,ncol=p,byrow=F)
    }
  return(x)
}
```

# Linear regression gradient descent

```{r echo = T}
N <- 100
p <- 10
rho <- 0.2
lin_eta <- 0.08
lin_precision <- 0.0002
lin_iters <-5000
X <-genx(N, p, rho)
w_true <- ((-1)^(1:p))*exp(-2*((1:p)-1)/20)
eps <-rnorm(N)
k <- 3
y <- X%*%w_true+k*eps
res <-GD(X, y, Ein_linreg, gEin_linreg,
         w0=rep(0, p), 
         eta=lin_eta, 
         precision=lin_precision, 
         nb_iters=lin_iters)
plot(res$cost)
print(w_true)
w_calc <- unlist(tail(res$allw, 1))
sprintf("Estimated with a learning rate of %f" , lin_eta)
sprintf("Precision threshold:%f, max iterations:%i", lin_precision, lin_iters)
print(w_calc)
```

# Generate closed form solution

```{r echo=T}
w_analytical <- (solve(t(X)%*%X)%*%t(X)%*%y)
print(w_analytical[,1])
```

```{r}
w_df <- data.frame(xvar = 1:10, w_true, w_calc, w_analytical) %>%
  pivot_longer(!xvar, names_to = "weights", values_to="value")

ggplot(w_df, aes(xvar,value)) + 
  geom_line(aes(colour = weights),size=2, alpha=0.5)
```

# Logistic regression gradient descent

```{r echo=T}
set.seed(1900)
N <- 100
l <--5; u <- 5
x <-seq(l, u, by = 0.1)
w_true <-matrix(c(-3, 1, 1), ncol = 1)
a <--w_true[2]/w_true[3]
b <--w_true[1]/w_true[3]
X0 <-matrix(runif(2*N, l, u), ncol = 2)
X <-cbind(1, X0)
y <-sign(X%*%w_true)
```

```{r echo=F}
log_eta <- 0.08
log_precision <- 0.0002
log_iters <-5000

res <-GD(X, y, Ein_logreg, gEin_logreg,          
         w0=c(0, 0, 0), 
         eta=log_eta, 
         precision=log_precision, 
         nb_iters=log_iters)
plot(res$cost)
print("The true weights are:")
print(w_true[,1])
w_best <-unlist(tail(res$allw,1))
print("The estimated weights are:")
print(w_best)
sprintf("Estimated with a learning rate of %f" , log_eta)
sprintf("Precision threshold:%f, max iterations:%i", log_precision, log_iters)
plot(c(l, u),c(u, l),type ='n',xlab ="x1", ylab ="x2")
lines(x, a*x+b)
points(X0,col =ifelse(y==1, "red", "blue"))
a_best <--w_best[2]/w_best[3]
b_best <--w_best[1]/w_best[3]
lines(x, a_best*x+b_best,col ="red")
```


# References
[1]‘Yaser S. Abu-Mostafa’. https://work.caltech.edu/index.html (accessed Sep. 15, 2020).
